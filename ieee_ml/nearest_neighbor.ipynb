{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor\n",
    "\n",
    "\n",
    "This function illustrates how to use k-nearest neighbors in tensorflow\n",
    "\n",
    "We will use the 1970s Boston housing dataset which is available through the UCI ML data repository. \n",
    "\n",
    "### Data:\n",
    "----------x-values-----------\n",
    "* CRIM   : per capita crime rate by town\n",
    "* ZN     : prop. of res. land zones\n",
    "* INDUS  : prop. of non-retail business acres\n",
    "* CHAS   : Charles river dummy variable\n",
    "* NOX    : nitrix oxides concentration / 10 M\n",
    "* RM     : Avg. # of rooms per building\n",
    "* AGE    : prop. of buildings built prior to 1940\n",
    "* DIS    : Weighted distances to employment centers\n",
    "* RAD    : Index of radian highway access\n",
    "* TAX    : Full tax rate value per $10k\n",
    "* PTRATIO: Pupil/Teacher ratio by town\n",
    "* B      : 1000*(Bk-0.63)^2, Bk=prop. of blacks\n",
    "* LSTAT  : % lower status of pop\n",
    "\n",
    "------------y-value-----------\n",
    "* MEDV   : Median Value of homes in $1,000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "#importing pyplot from matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "#importing tensorflow\n",
    "import tensorflow as tf\n",
    "#importing requests which will be used for fetching data\n",
    "import requests\n",
    "#Clearing the default graph stack and resetting the global default graph.\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "#importing debug library\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a session object which creates an environment where we can execute Operations and evaluate Tensors\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger\n",
    "\n",
    "### Uncomment the below line and execute the code to run the debugger.\n",
    "\n",
    "### Go to the link once you start execution    \t\t\thttp://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the below line to run the debugger\n",
    "sess = tf_debug.TensorBoardDebugWrapperSession(sess, \"localhost:6064\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='archive.ics.uci.edu', port=443): Max retries exceeded with url: /ml/machine-learning-databases/housing/housing.data (Caused by SSLError(SSLEOFError(8, u'EOF occurred in violation of protocol (_ssl.c:590)'),))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mSSLError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c182bbb984e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_used\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Using requests, load the url containing the dataset and fetch the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhousing_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#For each line withe length greater than 0, split the line based on space and store it in a 2-d list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhousing_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhousing_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    510\u001b[0m         }\n\u001b[1;32m    511\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='archive.ics.uci.edu', port=443): Max retries exceeded with url: /ml/machine-learning-databases/housing/housing.data (Caused by SSLError(SSLEOFError(8, u'EOF occurred in violation of protocol (_ssl.c:590)'),))"
     ]
    }
   ],
   "source": [
    "#URL for the boston housing data in UCI repository is set to the variable\n",
    "if os.path.exists('housing.data.dat'):\n",
    "    housing_file=pickle.load(open('housing.data', 'rb'))\n",
    "else:\n",
    "    housing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "    #the different features in the dataset\n",
    "    housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "    #the features being used in our model\n",
    "    cols_used = ['CRIM', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "    #The number of features being used in the model\n",
    "    num_features = len(cols_used)\n",
    "    #Using requests, load the url containing the dataset and fetch the data\n",
    "    housing_file = requests.get(housing_url)\n",
    "    pickle.dump(housing_file,open('housing.data', 'wb') ) \n",
    "    #For each line withe length greater than 0, split the line based on space and store it in a 2-d list\n",
    "housing_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in housing_file.text.split('\\n') if len(y)>=1]\n",
    "\n",
    "#retrieve the 13th value in each row of the 2-d list, convert it to a numpy array, take transpose to obtain n x 1 array\n",
    "y_vals = np.transpose([np.array([y[13] for y in housing_data])])\n",
    "#retrieve the data belonging to the features to be used in the model, convert it to a 2-d numpy array\n",
    "x_vals = np.array([[x for i,x in enumerate(y) if housing_header[i] in cols_used] for y in housing_data])\n",
    "\n",
    "## Min-Max Scaling\n",
    "#Normalize the data\n",
    "x_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeding a pseudo-random number generator to give it its first previous value, making future generations reproducible\n",
    "np.random.seed(13)  #make results reproducible\n",
    "#generate a random list of indices having 80% of the original indices, use it to form the train data\n",
    "train_indices = np.random.choice(len(x_vals), int(round(len(x_vals)*0.8)), replace=False)\n",
    "#use the indices generated in the previous step and retrieve the remaining 20% of the indices for test data\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "#using the list of indices genereated obtain the training data from x_vals\n",
    "x_vals_train = x_vals[train_indices]\n",
    "#using the list of test indices generated obtain the testing data from x_vals\n",
    "x_vals_test = x_vals[test_indices]\n",
    "#use the same set of train indices and obtain the corresponding labels for the train data\n",
    "y_vals_train = y_vals[train_indices]\n",
    "#use the same set of test indices and obtain the corresponding lables for the test data\n",
    "y_vals_test = y_vals[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to control run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare k-value and batch size\n",
    "#Setting the k value, used in the calulation of nearest neighbors\n",
    "k = 4\n",
    "#set the batch size to length of test data, number of training samples to be used in one iteration\n",
    "batch_size=len(x_vals_test)\n",
    "\n",
    "# Placeholders\n",
    "#Inserting a placeholder for a tensor of size train data\n",
    "x_data_train = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "#Inserting a placeholder for a tensor of size test data\n",
    "x_data_test = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "#Inserting a placeholder for a tensor of labels size of train data\n",
    "y_target_train = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "#Inserting a placeholder for a tensor of labels size of test data\n",
    "y_target_test = tf.placeholder(shape=[None, 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare distance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Distance Metric\n",
    "\n",
    "Uncomment following line and comment L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following line calculates the distance matrix using manhattan distance metric\n",
    "#the distance is calulated between each test point and all train data points\n",
    "distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Distance Metric\n",
    "\n",
    "Uncomment following line and comment L1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following line calculates the distance matrix using eucledian distance metric\n",
    "#the distance is calulated between each test point and all train data points\n",
    "#distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict: Get min distance index (Nearest neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = tf.arg_min(distance, 0)\n",
    "#Finds values and indices of the k largest entries for the negative values of distance matrix (smallest positive)\n",
    "top_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\n",
    "#reduce the tensor to a 1-d tensor (flatten) and add the second dimension\n",
    "x_sums = tf.expand_dims(tf.reduce_sum(top_k_xvals, 1),1)\n",
    "#creating a matrix of 1's of size 1xk and multiplying it with the sum matrix generated in previous steps\n",
    "x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\n",
    "#dividing each value in top_k_val matrix with correspoding value from x_sums repeated and expanding its dimension by 1\n",
    "x_val_weights = tf.expand_dims(tf.div(top_k_xvals,x_sums_repeated), 1)\n",
    "\n",
    "\n",
    "#retrieve the labels belonging to the top_k_indices\n",
    "top_k_yvals = tf.gather(y_target_train, top_k_indices)\n",
    "\n",
    "#multiply the caluclated weights with the respective labels and reduce the dimension of tensor by 1\n",
    "prediction = tf.squeeze(tf.matmul(x_val_weights,top_k_yvals), axis=[1])\n",
    "#prediction = tf.reduce_mean(top_k_yvals, 1)\n",
    "\n",
    "\n",
    "# Calculate MSE\n",
    "#calculating mean square error for the predicted labels and divide it by the batch size\n",
    "mse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)\n",
    "\n",
    "\n",
    "# Calculate how many loops over training data\n",
    "num_loops = int(np.ceil(len(x_vals_test)/batch_size))\n",
    "\n",
    "#iterating for the training data for num_loops\n",
    "for i in range(num_loops):\n",
    "    #starting index of the current batch\n",
    "    min_index = i*batch_size\n",
    "    #ending index of the current batch\n",
    "    max_index = min((i+1)*batch_size,len(x_vals_train))\n",
    "    #data for testing of batch size\n",
    "    x_batch = x_vals_test[min_index:max_index]\n",
    "    #labels for the test data of batch size\n",
    "    y_batch = y_vals_test[min_index:max_index]\n",
    "    #run the graph fragment to execute the operation (predcition) and evaluate each tensor using data from feed_dict\n",
    "    predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n",
    "                                         y_target_train: y_vals_train, y_target_test: y_batch})\n",
    "    #run the graph fragment to execute the operation (calculate mse) and evaluate each tensor using data from feed_dict\n",
    "    batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n",
    "                                         y_target_train: y_vals_train, y_target_test: y_batch})\n",
    "\n",
    "    #print the mse for the current batch\n",
    "    print('Batch #' + str(i+1) + ' MSE: ' + str(np.round(batch_mse,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output of plotting commands is displayed inline within frontends, stored in notebook\n",
    "%matplotlib inline\n",
    "# Plot prediction and actual distribution\n",
    "#store 45 evenly spaced numbers between 5 and 50\n",
    "bins = np.linspace(5, 50, 45)\n",
    "\n",
    "#plot the histogram for predicted values\n",
    "plt.hist(predictions, bins, alpha=0.5, label='Prediction')\n",
    "#plot the histogram for actual values\n",
    "plt.hist(y_batch, bins, alpha=0.5, label='Actual')\n",
    "#set title for the histogram\n",
    "plt.title('Histogram of Predicted and Actual Values')\n",
    "#labeling the x-axis of the plot\n",
    "plt.xlabel('Med Home Value in $1,000s')\n",
    "#labeling the y-axis of the plot\n",
    "plt.ylabel('Frequency')\n",
    "#set the location of the legend on the plot\n",
    "plt.legend(loc='upper right')\n",
    "#display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
